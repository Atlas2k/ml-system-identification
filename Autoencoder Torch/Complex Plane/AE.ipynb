{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Training and Test Data from Local Directory\n",
    "\n",
    "Training and test data is loaded from two local .mat files and are formatted into numpy ndarrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = sio.loadmat('DS.mat')\n",
    "images = images['DS']\n",
    "tr_images = images[:800, :, :, :]\n",
    "test_images = images[800:, :, :, :]\n",
    "labels = sio.loadmat('labels.mat')\n",
    "labels = labels['par']\n",
    "tr_labels = labels[:800, :]\n",
    "test_labels = labels[800:, :]\n",
    "# Change the format of tensor since in torch channels are in the second axis.\n",
    "tr_images = np.moveaxis(tr_images, -1, 1)\n",
    "test_images = np.moveaxis(test_images, -1, 1)\n",
    "print(np.shape(tr_images), np.shape(test_images))\n",
    "print(np.shape(tr_labels), np.shape(test_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the pyTorch Image Dataset Class\n",
    "\n",
    "In this cell we define an \"image dataset\" type to be used as the input to our pyTorch CNN model.\n",
    "\n",
    "Class methods like get, constructor... that are required for the dataset to function are defined for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TF_image_dataset(Dataset):\n",
    "    def __init__(self, in_dataset, out_dataset, transform=None):\n",
    "        self.in_dataset = torch.Tensor(in_dataset)\n",
    "        self.out_dataset = torch.Tensor(out_dataset)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.in_dataset.size(0)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.in_dataset[index]\n",
    "        x = self.transform(x)\n",
    "        y = self.out_dataset[index]\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def get_loader(in_dataset, out_dataset, batch_size, transform, num_workers, pin_memory=True):\n",
    "    tr_ds = TF_image_dataset(\n",
    "        in_dataset=in_dataset,\n",
    "        out_dataset=out_dataset,\n",
    "        transform=transform,\n",
    "    )\n",
    "    train_loader = DataLoader(tr_ds,\n",
    "                              batch_size=batch_size,\n",
    "                              num_workers=num_workers,\n",
    "                              pin_memory=pin_memory,\n",
    "                              shuffle=True,\n",
    "                              )\n",
    "\n",
    "    return train_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Declare and Construct Loader for the Encoder Decoder Model\n",
    "\n",
    "We declare and build the final input data to our CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 15\n",
    "NUM_WORKERS = 0\n",
    "PIN_MEMORY = True\n",
    "TRANSFORM = transforms.Compose(\n",
    "    [\n",
    "        transforms.Normalize(mean=[0, 0], std=[50, 3.14])]\n",
    ")\n",
    "\n",
    "train_loader = get_loader(tr_images, tr_labels,\n",
    "                          BATCH_SIZE, TRANSFORM, NUM_WORKERS, PIN_MEMORY)\n",
    "test_loader = get_loader(test_images, test_labels, 200,\n",
    "                         TRANSFORM, NUM_WORKERS, PIN_MEMORY)\n",
    "imag, l = next(iter(train_loader))\n",
    "print(imag.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Visualization\n",
    "\n",
    "This cell prints a batch of samples from the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 30))\n",
    "for i in range(BATCH_SIZE):\n",
    "    ax = plt.subplot(5, 5, i+1)\n",
    "    plt.imshow(imag[i, 1, :, :], cmap='gray')\n",
    "    plt.axis('off')\n",
    "    # print(torch.max(imag[i,0,:,:]),torch.min(imag[i,0,:,:])) # Printing the max and min value in the plotted samples\n",
    "    # print(torch.mean(imag[i,0,:,:]),torch.mean(imag[i,0,:,:])) # Printing the mean value in the plotted samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Encoder Decoder CNN\n",
    "\n",
    "A pyTorch NN class and it's forward method is defined that will be used as the Encoder Decoder model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # inputsize--> N, 2, 200, 200\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(2, 8, 10, stride=5),  # --> N, 8, 39, 39\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(8, 16, 5, stride=2),  # --> N, 16, 18, 18\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(16, 32, 5),  # --> N, 32, 14, 14\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 14)  # --> N, 64, 1, 1\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, 14),  # --> N, 32, 14, 14\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 16, 5),  # --> N, 16, 18, 18\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 8, 10, stride=3),  # --> N, 8, 61, 61\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(8, 2, 20, stride=3),  # --> N, 8, 101, 101\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining CNN Cost Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Autoencoder()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Encoder Decoder Model\n",
    "\n",
    "We now use the training dataset that contains our input images and labels to train the encoder decoder model to reconstruct the images in the input to the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 15\n",
    "for epoch in range(num_epochs):\n",
    "    for (img, _) in train_loader:\n",
    "        recon = model(img)\n",
    "        loss = criterion(recon, img)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Epoch:{epoch+1}, Loss:{loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch and Plot 5 Input Samples and Corresponding Reconstructed Outputs \n",
    "\n",
    "Sanity check of the validity and quality of the encoder decoder model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch 5 input and output samples\n",
    "out = []\n",
    "for (img, _) in train_loader:\n",
    "    recon = model(img)\n",
    "    out.append((img, recon))\n",
    "print(type(img), type(recon))\n",
    "print(img.size(), recon.size())\n",
    "\n",
    "# Plot the 5 input samples\n",
    "plt.figure(figsize=(30, 30))\n",
    "for i in range(5):\n",
    "    ax = plt.subplot(5, 5, i+1)\n",
    "    plt.imshow(img.numpy()[i, 1, :, :], cmap='gray')\n",
    "    plt.axis('off')\n",
    "\n",
    "# Plot the 5 corresponding output samples\n",
    "plt.figure(figsize=(30, 30))\n",
    "for i in range(5):\n",
    "    ax = plt.subplot(5, 5, i+1)\n",
    "    plt.imshow(recon.detach().numpy()[i, 1, :, :], cmap='gray')\n",
    "    plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Latent Variables\n",
    "\n",
    "We extract the latent variables from the output of the encoder in our model. Two lists are created which house the variables and their labels (System parameters) for each of the training and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting latent variables for the training set\n",
    "x_FC, y_FC = [], []\n",
    "\n",
    "for (img, L) in train_loader:\n",
    "    latent = model.encoder(img)\n",
    "    latent = latent.squeeze()\n",
    "    x_FC.append(latent)\n",
    "    y_FC.append(L)\n",
    "\n",
    "x_FC = torch.cat(x_FC, axis=0)\n",
    "x_FC = x_FC.detach()\n",
    "y_FC = torch.cat(y_FC, axis=0)\n",
    "\n",
    "# Getting latent variables for the training set\n",
    "test_x_FC, test_y_FC = [], []\n",
    "\n",
    "print(x_FC.size())\n",
    "for (img, L) in test_loader:\n",
    "    latent = model.encoder(img)\n",
    "    latent = latent.squeeze()\n",
    "    test_x_FC.append(latent)\n",
    "    test_y_FC.append(L)\n",
    "\n",
    "test_x_FC = torch.cat(test_x_FC, axis=0)\n",
    "test_x_FC = test_x_FC.detach()\n",
    "test_y_FC = torch.cat(test_y_FC, axis=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Fully Connected NN at the Output\n",
    "\n",
    "A pyTorch NN class and it's forward method is defined that will be used as the Fully Connected NN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC (nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.NN = nn.Sequential(\n",
    "            nn.Linear(64, 32),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(16, 6)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.NN(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining FCC Cost Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = FC()\n",
    "criterion2 = nn.MSELoss()\n",
    "optimizer2 = optim.Adam(model2.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Fully Connected Model \n",
    "\n",
    "We now use the training dataset that contains our latent variables and their labels to train the FCC to get the system parameters from information that was extracted from input images through the latent variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_FC.size())\n",
    "\n",
    "num_epochs = 10000\n",
    "for epoch in range(num_epochs):\n",
    "    pred = model2(x_FC)\n",
    "    loss = criterion2(pred, y_FC)\n",
    "    optimizer2.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer2.step()\n",
    "    print(f'Epoch:{epoch+1}, Loss:{loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Result Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model2(test_x_FC)\n",
    "\n",
    "# Scatter plot\n",
    "plt.figure(figsize=(12, 10), dpi=80)\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.scatter(test_y_FC.numpy()[:, i], pred.detach().numpy()[:, i])\n",
    "\n",
    "# Histogram\n",
    "plt.figure(figsize=(12, 10), dpi=80)\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.hist((test_y_FC.numpy()[:, i]-pred.detach().numpy()\n",
    "             [:, i])/test_y_FC.numpy()[:, i]*100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
